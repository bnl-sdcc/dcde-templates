{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# DCDE 11/14/19 dry run\n",
    "\n",
    "## Topics\n",
    "  * prerequisites\n",
    "  * recipes, templates\n",
    "  * lessons learned\n",
    "  * components we're using\n",
    "  * Participants\n",
    "  * How accounts are created\n",
    "  * Live demo\n",
    "\n",
    "\n",
    "## Demo steps  \n",
    "\n",
    "  - Prepare for demo\n",
    "    - Log out all the things\n",
    "    - Activate Globus endpoints\n",
    "  - Demo begins\n",
    "    - Log in to jupyterhub via cilogon\n",
    "    - Talk about single DCDE identity used across sites, how they're set up for it\n",
    "    - Talk about 3 sites participating - DCDE set up w/ oauth_ssh, Globus, mix of Condor & Slurm\n",
    "    - Load demo page\n",
    "    - Introduce Relion - say we're using containers, singlarity at each site\n",
    "    - Show data set at bnl - say we've got it staged to other prerequisites\n",
    "    - Talk about parsl -- we're leveraging it to run across a distributed,  mixed environment\n",
    "    - Run import(?) at BNL.  \n",
    "    - Sync data out to ORNL for (motioncorr or ctffind) -- something quick\n",
    "    - Sync data back to BNL via Globus\n",
    "    - Sync data out to ANL\n",
    "    - Run autopick 3d refine(?) at  anl\n",
    "    - pull data back to bnl\n",
    "    - Show pictures w/ nglview at BNL\n",
    "\n",
    "*note: clean relion dataset is at gssh.lcrc.anl.gov:/home/dcowley/dcde-sc19-relion-data-clean.tgz.  There may be copies elsewhere...*\n",
    "\n",
    "## list of steps/outputs:\n",
    "\n",
    "| Target Site | job step | output type |  Output treatment | Approx. time|\n",
    "| ----- | -----  | ----- | ----- | ---|\n",
    "| ORNL |autopick | | | | |\n",
    "| ORNL |extract | | | |\n",
    "| ? | ctffind |  | | |\n",
    "| ? | autopick |  | | |\n",
    "| ANL | 3d refine? - view w/ NGL |   | | | \n",
    "\n",
    "##  Investigate (see sc19-screenply-cruft.md):\n",
    "\n",
    "  * Did I do motioncor on Cascade w/ GPGPUs? I think I did.  \n",
    "    * Is unblur in the singularity container?\n",
    "  * Will relion_display work in any way?\n",
    "  * Can we have some canned pictures?  Capture shots from X display or Chimera or something\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Here is all the parsl/Globus setup \"\"\"\n",
    "\n",
    "\n",
    "import parsl\n",
    "import os\n",
    "from parsl.config import Config\n",
    "\n",
    "\n",
    "from parsl.channels import OAuthSSHChannel\n",
    "from parsl.providers import CondorProvider\n",
    "from parsl.providers import SlurmProvider\n",
    "from parsl.launchers import SrunLauncher\n",
    "from parsl.executors import HighThroughputExecutor\n",
    "from parsl.addresses import address_by_hostname\n",
    "from parsl.app.app import bash_app\n",
    "from parsl.app.app import python_app\n",
    "\n",
    "#parsl.set_stream_logger()\n",
    "\n",
    "anl_config = Config(\n",
    "    app_cache=True,\n",
    "    checkpoint_files=None,\n",
    "    checkpoint_mode=None,\n",
    "    checkpoint_period=None,\n",
    "    data_management_max_threads=10,\n",
    "    executors=[HighThroughputExecutor(\n",
    "        address='130.199.185.13',\n",
    "        cores_per_worker=1.0,\n",
    "        heartbeat_period=30,\n",
    "        heartbeat_threshold=120,\n",
    "        interchange_port_range=(50000, 51000),\n",
    "        label='anl-slurm',\n",
    "        launch_cmd='process_worker_pool.py {debug} {max_workers} -p {prefetch_capacity} -c {cores_per_worker} -m {mem_per_worker} --poll {poll_period} --task_url={task_url} --result_url={result_url} --logdir={logdir} --block_id={{block_id}} --hb_period={heartbeat_period} --hb_threshold={heartbeat_threshold} ',\n",
    "        managed=True,\n",
    "        max_workers=1,\n",
    "        #mem_per_worker=None,\n",
    "        poll_period=10,\n",
    "        prefetch_capacity=0,\n",
    "        interchange_address='10.70.128.9', #this is the address worker talk to inetrchange(head node)\n",
    "        provider=SlurmProvider(\n",
    "            'debug',\n",
    "            channel=OAuthSSHChannel(\n",
    "                'gssh.lcrc.anl.gov',\n",
    "                envs={},\n",
    "                port=2222,\n",
    "                script_dir='/home/dcowley/ornl-parsl-scripts',\n",
    "                username='dcowley'\n",
    "            ),\n",
    "            cmd_timeout=10,\n",
    "            exclusive=True,\n",
    "            init_blocks=1,\n",
    "            # launcher=SingleNodeLauncher(),\n",
    "            max_blocks=1,\n",
    "            min_blocks=1,\n",
    "            move_files=True,\n",
    "            nodes_per_block=1,\n",
    "            parallelism=0.0,\n",
    "            scheduler_options='#SBATCH -A dcde\\n#SBATCH -t 0:20:00\\n#SBATCH -N 1\\n#SBATCH --ntasks-per-node=36\\n#SBATCH -J relion-autopick\\n#SBATCH -p bdwall\\n#SBATCH -D /blues/gpfs/home/dcowley/relion-bootstrap\\n#SBATCH -o relion-autopick.%j.out\\n#SBATCH -e relion-autopick.%j.err',\n",
    "            walltime='00:10:00',\n",
    "            #worker_init='source /home/dcde1000001/dcdesetup.sh'\n",
    "            worker_init='source /lcrc/project/DCDE/setup.sh;  source activate /lcrc/project/DCDE/envs/dcdeRX; export I_MPI_FABRICS=shm:tmi'\n",
    "        ),\n",
    "        storage_access=[],\n",
    "        suppress_failure=False,\n",
    "        worker_debug=True,\n",
    "        worker_logdir_root='/home/dcowley/parsl_scripts/logs',\n",
    "        worker_port_range=(50000, 51000),\n",
    "        #worker_ports=None,\n",
    "        working_dir='/home/dcowley/parsl_scripts'\n",
    "    )],\n",
    "    lazy_errors=True,\n",
    "    monitoring=None,\n",
    "    retries=0,\n",
    "    run_dir='runinfo',\n",
    "    strategy='simple',\n",
    "    usage_tracking=False\n",
    ")\n",
    "\n",
    "bnl_config = Config(\n",
    "    app_cache=True,\n",
    "    checkpoint_files=None,\n",
    "    checkpoint_mode=None,\n",
    "    checkpoint_period=None,\n",
    "    data_management_max_threads=10,\n",
    "    executors=[HighThroughputExecutor(\n",
    "        #address='127.0.0.1',\n",
    "        address='130.199.185.13',\n",
    "        cores_per_worker=1,\n",
    "        heartbeat_period=30,\n",
    "        heartbeat_threshold=120,\n",
    "        interchange_port_range=(50000, 51000),\n",
    "        label='bnl-condor',\n",
    "        launch_cmd='process_worker_pool.py {debug} {max_workers} -p {prefetch_capacity} -c {cores_per_worker} -m {mem_per_worker} --poll {poll_period} --task_url={task_url} --result_url={result_url} --logdir={logdir} --block_id={{block_id}} --hb_period={heartbeat_period} --hb_threshold={heartbeat_threshold} ',\n",
    "        mem_per_worker=4,\n",
    "        managed=True,\n",
    "        max_workers=1,\n",
    "        poll_period=10,\n",
    "        prefetch_capacity=0,\n",
    "        interchange_address='130.199.185.9', #this is the address worker talk to inetrchange(head node)\n",
    "        provider=CondorProvider(\n",
    "            channel=OAuthSSHChannel(\n",
    "                'spce01.sdcc.bnl.gov',\n",
    "                envs={},\n",
    "                port=2222,\n",
    "                script_dir='/sdcc/u/dcde1000006/parsl_scripts',\n",
    "                username='dcde1000006'\n",
    "            ),\n",
    "            environment={},\n",
    "            init_blocks=1,\n",
    "            # launcher=SingleNodeLauncher(),\n",
    "            max_blocks=1,\n",
    "            min_blocks=1,\n",
    "            nodes_per_block=1,\n",
    "            #parallelism=1,\n",
    "            parallelism=0,\n",
    "            project='',\n",
    "            #Trying this Requirements directive per Dong's instructions:\n",
    "            #requirements='regexp(\"^sp[oa]\", machine)',\n",
    "            scheduler_options='accounting_group = group_sdcc.main \\nRequirements = (regexp(\"^sp[oa]\", machine))',\n",
    "            transfer_input_files=[],\n",
    "            walltime='00:30:00',\n",
    "            #worker_init='source /sdcc/u/dcde1000001/dcdesetup.sh'\n",
    "            worker_init='source /hpcgpfs01/work/dcde/setup.sh; source activate dcdeRX'\n",
    "        ),\n",
    "        storage_access=[],\n",
    "        suppress_failure=False,\n",
    "        worker_debug=True,\n",
    "        worker_logdir_root='/sdcc/u/dcde1000006/parsl_scripts/logs',\n",
    "        worker_port_range=(50000, 51000),\n",
    "        #worker_port_range=(5000, 5100),   # per John H's message 8/29/19\n",
    "        worker_ports=None,\n",
    "        working_dir='/sdcc/u/dcde1000006/parsl_scripts'\n",
    "    )],\n",
    "    lazy_errors=True,\n",
    "    monitoring=None,\n",
    "    retries=0,\n",
    "    run_dir='runinfo',\n",
    "    strategy='simple',\n",
    "    usage_tracking=False\n",
    ")\n",
    "\n",
    "ornl_config = Config(\n",
    "    app_cache=True,\n",
    "    checkpoint_files=None,\n",
    "    checkpoint_mode=None,\n",
    "    checkpoint_period=None,\n",
    "    data_management_max_threads=10,\n",
    "    executors=[HighThroughputExecutor(\n",
    "        address='130.199.185.13',\n",
    "        cores_per_worker=1.0,\n",
    "        heartbeat_period=30,\n",
    "        heartbeat_threshold=120,\n",
    "        interchange_port_range=(50000, 51000),\n",
    "        label='ornl-slurm',\n",
    "        launch_cmd='process_worker_pool.py {debug} {max_workers} -p {prefetch_capacity} -c {cores_per_worker} -m {mem_per_worker} --poll {poll_period} --task_url={task_url} --result_url={result_url} --logdir={logdir} --block_id={{block_id}} --hb_period={heartbeat_period} --hb_threshold={heartbeat_threshold} ',\n",
    "        managed=True,\n",
    "        max_workers=1,\n",
    "        #mem_per_worker=None,\n",
    "        poll_period=10,\n",
    "        prefetch_capacity=0,\n",
    "        interchange_address='128.219.185.39', #this is the address worker talk to inetrchange(head node)\n",
    "        provider=SlurmProvider(\n",
    "            'debug',\n",
    "            channel=OAuthSSHChannel(\n",
    "                'dcde-ext.ornl.gov',\n",
    "                envs={},\n",
    "                port=2222,\n",
    "                script_dir='/home/dcde1000006/ornl-parsl-scripts',\n",
    "                username='dcde1000006'\n",
    "            ),\n",
    "            cmd_timeout=10,\n",
    "            exclusive=True,\n",
    "            init_blocks=1,\n",
    "            # launcher=SingleNodeLauncher(),\n",
    "            max_blocks=1,\n",
    "            min_blocks=1,\n",
    "            move_files=True,\n",
    "            nodes_per_block=1,\n",
    "            parallelism=0.0,\n",
    "            scheduler_options='#SBATCH -D /nfs/scratch/relion-bootstrap\\n#SBATCH -o relion-autopick.%j.out\\n#SBATCH -e relion-autopick.%j.err',\n",
    "            walltime='00:10:00',\n",
    "            worker_init='source /nfs/scratch/dcde1000012/RX.sh'\n",
    "        ),\n",
    "        storage_access=[],\n",
    "        suppress_failure=False,\n",
    "        worker_debug=True,\n",
    "        worker_logdir_root='/nfs/scratch/dcde1000006/parsl_scripts/logs',\n",
    "        worker_port_range=(50000, 51000),\n",
    "        #worker_ports=None,\n",
    "        working_dir='/nfs/scratch/dcde1000006/parsl_scripts'\n",
    "    )],\n",
    "    lazy_errors=True,\n",
    "    monitoring=None,\n",
    "    retries=0,\n",
    "    run_dir='runinfo',\n",
    "    strategy='simple',\n",
    "    usage_tracking=False\n",
    ")\n",
    "\n",
    "ANL_EP = '57b72e31-9f22-11e8-96e1-0a6d4e044368'\n",
    "BNL_EP = '23f78cc8-41e0-11e9-a618-0a54e005f950'\n",
    "EMSL_EP = 'e133a52e-6d04-11e5-ba46-22000b92c6ec'\n",
    "ORNL_EP = '57230a10-7ba2-11e7-8c3b-22000b9923ef'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Set up Globus auth\"\"\"\n",
    "import subprocess\n",
    "import json\n",
    "from globus_sdk import (NativeAppAuthClient, TransferClient,\n",
    "                        RefreshTokenAuthorizer, TransferData)\n",
    "from globus_sdk.exc import GlobusAPIError\n",
    "\n",
    "authout = subprocess.run(['/usr/local/anaconda3/bin/parsl-globus-auth'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "print (authout.stdout)\n",
    "print (authout.stderr)\n",
    "# Perform a Globus directory transfer, reusing refresh tokens we've already obtained for PARSL.\n",
    "# Note this is NOT a PARSL transfer\n",
    "\n",
    "\n",
    "def load_tokens_from_file(filepath):\n",
    "    \"\"\"Load a set of saved tokens.\"\"\"\n",
    "    with open(filepath, 'r') as f:\n",
    "        tokens = json.load(f)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def save_tokens_to_file(filepath, tokens):\n",
    "    \"\"\"Save a set of tokens for later use.\"\"\"\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(tokens, f)\n",
    "\n",
    "\n",
    "def update_tokens_file_on_refresh(token_response):\n",
    "    \"\"\"\n",
    "    Callback function passed into the RefreshTokenAuthorizer.\n",
    "    Will be invoked any time a new access token is fetched.\n",
    "    \"\"\"\n",
    "    save_tokens_to_file(globus_tokens, token_response.by_resource_server)\n",
    "\n",
    "dcde_parsl_client_id = '8b8060fd-610e-4a74-885e-1051c71ad473'\n",
    "\n",
    "globus_tokens='/home/dcde1000006/.parsl/.globus.json'\n",
    "\n",
    "# First authorize using those refresh tokens:\n",
    "\n",
    "try:\n",
    "    tokens = load_tokens_from_file(globus_tokens)\n",
    "\n",
    "except:\n",
    "    print(\"Valid refresh tokens not found in {}.  Unable to authorize to Globus.  Exiting!\".format(globus_tokens))\n",
    "    sys.exit(-1)\n",
    "\n",
    "\n",
    "transfer_tokens = tokens['transfer.api.globus.org']\n",
    "\n",
    "try:\n",
    "    auth_client = NativeAppAuthClient(client_id=dcde_parsl_client_id)\n",
    "except:\n",
    "    print (\"ERROR: Globus NativeAppAuthClient() call failed!  Unable to obtain a Globus authorizer!\")\n",
    "    sys.exit(-1)\n",
    "\n",
    "authorizer = RefreshTokenAuthorizer(\n",
    "    transfer_tokens['refresh_token'],\n",
    "    auth_client,\n",
    "    access_token=transfer_tokens['access_token'],\n",
    "    expires_at=transfer_tokens['expires_at_seconds'],\n",
    "    on_refresh=update_tokens_file_on_refresh)\n",
    "\n",
    "try:\n",
    "    tc = TransferClient(authorizer=authorizer)\n",
    "except:\n",
    "    print (\"ERROR: TransferClient() call failed!  Unable to call the Globus transfer interface with the provided auth info!\")\n",
    "    sys.exit(-1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should have a transfer client with auth, We can set up one or many transfers.  Remember each TransferData object has a specific src/dest, and we need to build in a list of files/dirs with add_item().\n",
    "\n",
    "Now Check to see we have parsl configs loaded for remote execution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The DCDE\n",
    "\n",
    "(Talk about DCDE environment, single identity, use of multiple sites with that one identity.  Compute/data at 3 sites: ANL, BNL, ORNL.  Talk about use case: Observational science with computing requirements, specifically Cryo-electron microscopy.  We have a body of data, need to do various computing steps on it to go from raw data to 3d reconstructions of proteings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The application & demonstration\n",
    "\n",
    "(Talk about Relion, which processes the Cryo-EM data. We will run the application in a singularity container at multiple compute sites, using Globus to sync the data across sites.  The workflow will be controlled by the parsl library of python functions, which we will use to run distributed HPC-style jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demo step: Show data set at BNL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lht /hpcgpfs01/scratch/dcde1000006/sc19-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demo step: Run import at BNL**  *Note: The short bnl job that i have right now is relion_postprocess*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job setup: stdout = /sdcc/u/dcde1000006/parsl_scripts/relion-bnl-import.out\n",
      "stderr = /sdcc/u/dcde1000006/parsl_scripts/relion-bnl-import.err\n",
      "relion_import() invoked, now waiting...\n",
      "working directory: /hpcgpfs01/scratch/dcde1000006/sc19-data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsl.clear()\n",
    "\n",
    "#parsl.set_stream_logger()\n",
    "parsl.load(bnl_config)\n",
    "bnl_dfk = parsl.dfk()\n",
    "\n",
    "@bash_app\n",
    "def relion_import(job_dir=None, stdout=None, stderr=None, mock=False):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    mock : (Bool)\n",
    "       when mock=True\n",
    "    \"\"\"\n",
    "    cmd_line = '''#!/bin/bash -l\n",
    "\n",
    "export DATAROOT=/hpcgpfs01/scratch/dcde1000006/sc19-data\n",
    "export RELION_SIMG=/sdcc/u/dcde1000006/relion_singv26.simg\n",
    "\n",
    "export MOVIESTAR=${{DATAROOT}}/Import/job001/movies.star\n",
    "export INSTAR=${{DATAROOT}}/CtfFind/job003/micrographs_ctf.star\n",
    "export REFSTAR=${{DATAROOT}}/Select/job007/class_averages.star\n",
    "export PICKDIR=${{DATAROOT}}/AutoPick/job010/\n",
    "\n",
    "cd ${{DATAROOT}}\n",
    "echo -n \"working directory: \"\n",
    "pwd\n",
    "#set -v\n",
    "\n",
    "singularity exec  -B /hpcgpfs01:/hpcgpfs01 ${{RELION_SIMG}} relion_star_loopheader rlnMicrographMovieName > ${{MOVIESTAR}}\n",
    "singularity exec  -B /hpcgpfs01:/hpcgpfs01 ${{RELION_SIMG}} ls Micrographs/*.mrcs >> ${{MOVIESTAR}}\n",
    "    '''\n",
    "    if mock:\n",
    "        return '''tmp_file=$(mktemp);\n",
    "cat<<EOF > $tmp_file\n",
    "{}\n",
    "EOF\n",
    "cat $tmp_file\n",
    "        '''.format(cmd_line)\n",
    "    else:\n",
    "        return cmd_line\n",
    "\n",
    "\n",
    "relion_stdout=os.path.join(bnl_config.executors[0].working_dir, 'relion-bnl-import.out')\n",
    "relion_stderr=os.path.join( bnl_config.executors[0].working_dir, 'relion-bnl-import.err')\n",
    "\n",
    "local_logdir='/hpcgpfs01/scratch/dcde1000006/sc19-data/parsl-outputs'\n",
    "local_logfile=os.path.join(local_logdir, 'relion-bnl-import.out')\n",
    "\n",
    "try:\n",
    "    os.remove(relion_stdout)\n",
    "except OSError:\n",
    "    pass\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "try:\n",
    "    os.remove(relion_stderr)\n",
    "except OSError:\n",
    "    pass\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "try:\n",
    "    os.remove(local_logfile)\n",
    "except OSError:\n",
    "    pass\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "\n",
    "print ('job setup: stdout = {}\\nstderr = {}'.format(relion_stdout,relion_stderr))\n",
    "# parsl.set_stream_logger()\n",
    "\n",
    "x = relion_import(job_dir=bnl_config.executors[0].working_dir, stdout=relion_stdout, stderr=relion_stderr, mock = False )\n",
    "print('relion_import() invoked, now waiting...')\n",
    "x.result()\n",
    "\n",
    "if x.done():\n",
    "    bnl_dfk.executors['bnl-condor'].provider.channel.pull_file(relion_stdout, local_logdir)\n",
    "    with open(local_logfile, 'r') as f:\n",
    "        print(f.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lt /hpcgpfs01/scratch/dcde1000006/sc19-data/Import/job001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demo step: Sync data from BNL to ORNL via Globus**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demo step: Run extract & autopick steps at ORNL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demo step: Sync new data back from ORNL to BNL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demo step: Sync new data back from ORNL to BNL, sync BNL to ANL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demo step: Sync new data back from ORNL to BNL, sync BNL to ANL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demo step: (Skipping ahead) run 3d refinement (parallel?) at ANL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demo step: Sync new data back from ANL to BNL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demo step: Show 3d reconstruction in nglview**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move me: Sync fresh data set from ANL to BNL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tdata = TransferData(tc, ANL_EP, BNL_EP,\n",
    "                     label=\"DCDE Relion transfer\",\n",
    "                     sync_level=\"size\")\n",
    "\n",
    "tdata.add_item('/blues/gpfs/home/dcowley/dcde-sc19-relion-data-clean.tgz',\n",
    "            '/hpcgpfs01/scratch/dcde1000006/dcde-sc19-relion-data-clean.tgz')\n",
    "\n",
    "transfer_result = tc.submit_transfer(tdata)\n",
    "\n",
    "print(\"task_id =\", transfer_result[\"task_id\"])\n",
    "\n",
    "\n",
    "while not tc.task_wait(transfer_result['task_id'], timeout=1200, polling_interval=10):\n",
    "    print(\".\", end=\"\")\n",
    "print(\"\\n{} completed!\".format(transfer_result['task_id']))\n",
    "\n",
    "os.listdir(path='/hpcgpfs01/scratch/dcde1000006/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l /hpcgpfs01/scratch/dcde1000006/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DCDE_RX",
   "language": "python",
   "name": "dcde_rx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
