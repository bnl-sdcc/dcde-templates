{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo objectives:\n",
    "\n",
    "## Demonstrate computation & workflow across a mixed environment:\n",
    "##  * From a single pane of glass\n",
    "##  * Across multiple computing sites\n",
    "##  * Different resource managers\n",
    "##  * Different storage resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Motivation: Computation Needed for Experimental Data\n",
    "## * Data comes from instrument\n",
    "## * Process data on  compute resources at multiple sites\n",
    "##    * Multi-step sequential workflow\n",
    "##    * Move data site-to-site as needed\n",
    "\n",
    "<img src=\"../../images/usecase-cartoon.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Tools used\n",
    "\n",
    "## Compute Resources:\n",
    "### * ANL LCRC + DTN \n",
    "### * BNL SDCC + DTN \n",
    "### * ORNL CADES + DTN\n",
    "\n",
    "## Software & Infrastructure:\n",
    "### * InCommon, CILogon, CoManage\n",
    "### * Globus & Oauth_ssh\n",
    "### * Jupyter Notebooks\n",
    "### * Parsl\n",
    "### * Singularity\n",
    "\n",
    "\n",
    "| Task | Tool |\n",
    "| ---- | ---- | \n",
    "| Identity Management | InCommon, CILogon, CoManage |\n",
    "| Authentication | Globus & Oauth_ssh |\n",
    "| User Interface | Jupyter Notebooks |\n",
    "| Data movement | Globus & Parsl |\n",
    "| Job submission | Parsl |\n",
    "| Machine virtualization | Singularity |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up parsl executors at remote sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of Sites & Steps\n",
    "\n",
    "\n",
    " |  Target Site |  Job Step |\n",
    " | ---- | ---- |\n",
    " | **BNL** | **Import** |\n",
    " | **ORNL** | **Autopick** |\n",
    " | **ANL** | **Extract** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import parsl\n",
    "import os\n",
    "from parsl.config import Config\n",
    "\n",
    "\n",
    "from parsl.channels import OAuthSSHChannel\n",
    "from parsl.providers import CondorProvider\n",
    "from parsl.providers import SlurmProvider\n",
    "from parsl.launchers import SrunLauncher\n",
    "from parsl.executors import HighThroughputExecutor\n",
    "from parsl.addresses import address_by_hostname\n",
    "from parsl.app.app import bash_app\n",
    "from parsl.app.app import python_app\n",
    "\n",
    "\n",
    "\n",
    "anl_config = Config(\n",
    "    app_cache=True,\n",
    "    checkpoint_files=None,\n",
    "    checkpoint_mode=None,\n",
    "    checkpoint_period=None,\n",
    "    data_management_max_threads=10,\n",
    "    executors=[HighThroughputExecutor(\n",
    "        address='130.199.185.13',\n",
    "        cores_per_worker=1.0,\n",
    "        heartbeat_period=30,\n",
    "        heartbeat_threshold=120,\n",
    "        interchange_port_range=(50000, 51000),\n",
    "        label='anl-slurm',\n",
    "        launch_cmd='process_worker_pool.py {debug} {max_workers} -p {prefetch_capacity} -c {cores_per_worker} -m {mem_per_worker} --poll {poll_period} --task_url={task_url} --result_url={result_url} --logdir={logdir} --block_id={{block_id}} --hb_period={heartbeat_period} --hb_threshold={heartbeat_threshold} ',\n",
    "        managed=True,\n",
    "        max_workers=1,\n",
    "        #mem_per_worker=None,\n",
    "        poll_period=10,\n",
    "        prefetch_capacity=0,\n",
    "        interchange_address='10.70.128.9', #this is the address worker talk to inetrchange(head node)\n",
    "        provider=SlurmProvider(\n",
    "            'debug',\n",
    "            channel=OAuthSSHChannel(\n",
    "                'gssh.lcrc.anl.gov',\n",
    "                envs={},\n",
    "                port=2222,\n",
    "                script_dir='/home/dcowley/anl-parsl-scripts',\n",
    "                username='dcowley'\n",
    "            ),\n",
    "            cmd_timeout=10,\n",
    "            exclusive=True,\n",
    "            init_blocks=1,\n",
    "            # launcher=SingleNodeLauncher(),\n",
    "            max_blocks=1,\n",
    "            min_blocks=1,\n",
    "            move_files=True,\n",
    "            nodes_per_block=1,\n",
    "            parallelism=0.0,\n",
    "            scheduler_options='#SBATCH -A dcde\\n#SBATCH -t 0:20:00\\n#SBATCH -N 1\\n#SBATCH --ntasks-per-node=36\\n#SBATCH -J relion-autopick\\n#SBATCH -p bdwall\\n#SBATCH -D /blues/gpfs/home/dcowley/sc19-demo\\n#SBATCH -o relion-autopick.%j.out\\n#SBATCH -e relion-autopick.%j.err',\n",
    "            walltime='00:10:00',\n",
    "            #worker_init='source /home/dcde1000001/dcdesetup.sh'\n",
    "            worker_init='source /lcrc/project/DCDE/setup.sh;  source activate /lcrc/project/DCDE/envs/dcdeRX; export I_MPI_FABRICS=shm:tmi'\n",
    "        ),\n",
    "        storage_access=[],\n",
    "        suppress_failure=False,\n",
    "        worker_debug=True,\n",
    "        worker_logdir_root='/home/dcowley/parsl_scripts/logs',\n",
    "        worker_port_range=(50000, 51000),\n",
    "        #worker_ports=None,\n",
    "        working_dir='/home/dcowley/parsl_scripts'\n",
    "    )],\n",
    "    lazy_errors=True,\n",
    "    monitoring=None,\n",
    "    retries=0,\n",
    "    run_dir='runinfo',\n",
    "    strategy='simple',\n",
    "    usage_tracking=False\n",
    ")\n",
    "\n",
    "print(\"ANL Parsl config loaded.\")\n",
    "\n",
    "bnl_config = Config(\n",
    "    app_cache=True,\n",
    "    checkpoint_files=None,\n",
    "    checkpoint_mode=None,\n",
    "    checkpoint_period=None,\n",
    "    data_management_max_threads=10,\n",
    "    executors=[HighThroughputExecutor(\n",
    "        #address='127.0.0.1',\n",
    "        address='130.199.185.13',\n",
    "        cores_per_worker=1,\n",
    "        heartbeat_period=30,\n",
    "        heartbeat_threshold=120,\n",
    "        interchange_port_range=(50000, 51000),\n",
    "        label='bnl-condor',\n",
    "        launch_cmd='process_worker_pool.py {debug} {max_workers} -p {prefetch_capacity} -c {cores_per_worker} -m {mem_per_worker} --poll {poll_period} --task_url={task_url} --result_url={result_url} --logdir={logdir} --block_id={{block_id}} --hb_period={heartbeat_period} --hb_threshold={heartbeat_threshold} ',\n",
    "        mem_per_worker=4,\n",
    "        managed=True,\n",
    "        max_workers=1,\n",
    "        poll_period=10,\n",
    "        prefetch_capacity=0,\n",
    "        interchange_address='130.199.185.9', #this is the address worker talk to inetrchange(head node)\n",
    "        provider=CondorProvider(\n",
    "            channel=OAuthSSHChannel(\n",
    "                'spce01.sdcc.bnl.gov',\n",
    "                envs={},\n",
    "                port=2222,\n",
    "                script_dir='/sdcc/u/dcde1000006/parsl_scripts',\n",
    "                username='dcde1000006'\n",
    "            ),\n",
    "            environment={},\n",
    "            init_blocks=1,\n",
    "            # launcher=SingleNodeLauncher(),\n",
    "            max_blocks=1,\n",
    "            min_blocks=1,\n",
    "            nodes_per_block=1,\n",
    "            #parallelism=1,\n",
    "            parallelism=0,\n",
    "            project='',\n",
    "            #Trying this Requirements directive per Dong's instructions:\n",
    "            #requirements='regexp(\"^sp[oa]\", machine)',\n",
    "            scheduler_options='accounting_group = group_sdcc.main \\nRequirements = (regexp(\"^sp[oa]\", machine))',\n",
    "            transfer_input_files=[],\n",
    "            walltime='00:30:00',\n",
    "            #worker_init='source /hpcgpfs01/work/dcde/setup.sh; source activate dcdemaster20191008'\n",
    "            worker_init='source /hpcgpfs01/work/dcde/setup.sh; source activate dcdeRX'\n",
    "        ),\n",
    "        storage_access=[],\n",
    "        suppress_failure=False,\n",
    "        worker_debug=True,\n",
    "        worker_logdir_root='/sdcc/u/dcde1000006/parsl_scripts/logs',\n",
    "        worker_port_range=(50000, 51000),\n",
    "        #worker_port_range=(5000, 5100),   # per John H's message 8/29/19\n",
    "        worker_ports=None,\n",
    "        working_dir='/sdcc/u/dcde1000006/parsl_scripts'\n",
    "    )],\n",
    "    lazy_errors=True,\n",
    "    monitoring=None,\n",
    "    retries=0,\n",
    "    run_dir='runinfo',\n",
    "    strategy='simple',\n",
    "    usage_tracking=False\n",
    ")\n",
    "\n",
    "print(\"BNL Parsl config loaded.\")\n",
    "\n",
    "ornl_config = Config(\n",
    "    app_cache=True,\n",
    "    checkpoint_files=None,\n",
    "    checkpoint_mode=None,\n",
    "    checkpoint_period=None,\n",
    "    data_management_max_threads=10,\n",
    "    executors=[HighThroughputExecutor(\n",
    "        address='130.199.185.13',\n",
    "        cores_per_worker=1.0,\n",
    "        heartbeat_period=30,\n",
    "        heartbeat_threshold=120,\n",
    "        interchange_port_range=(50000, 51000),\n",
    "        label='ornl-slurm',\n",
    "        launch_cmd='process_worker_pool.py {debug} {max_workers} -p {prefetch_capacity} -c {cores_per_worker} -m {mem_per_worker} --poll {poll_period} --task_url={task_url} --result_url={result_url} --logdir={logdir} --block_id={{block_id}} --hb_period={heartbeat_period} --hb_threshold={heartbeat_threshold} ',\n",
    "        managed=True,\n",
    "        max_workers=1,\n",
    "        #mem_per_worker=None,\n",
    "        poll_period=10,\n",
    "        prefetch_capacity=0,\n",
    "        interchange_address='128.219.185.39', #this is the address worker talk to interchange (head node)\n",
    "        provider=SlurmProvider(\n",
    "            'debug',\n",
    "            channel=OAuthSSHChannel(\n",
    "                'dcde-ext.ornl.gov',\n",
    "                envs={},\n",
    "                port=2222,\n",
    "                #script_dir='/home/dcde1000006/ornl-parsl-scripts',\n",
    "                script_dir='/nfs/scratch/dcde1000006/ornl-parsl-scripts',\n",
    "                username='dcde1000006'\n",
    "            ),\n",
    "            cmd_timeout=10,\n",
    "            exclusive=True,\n",
    "            init_blocks=1,\n",
    "            # launcher=SingleNodeLauncher(),\n",
    "            max_blocks=1,\n",
    "            min_blocks=1,\n",
    "            move_files=True,\n",
    "            nodes_per_block=1,\n",
    "            parallelism=0.0,\n",
    "            scheduler_options='#SBATCH -D /nfs/scratch/sc19-data\\n#SBATCH -o relion-autopick.%j.out\\n#SBATCH -e relion-autopick.%j.err',\n",
    "            walltime='00:10:00',\n",
    "            worker_init='source /nfs/scratch/dcde1000012/RX.sh'\n",
    "        ),\n",
    "        storage_access=[],\n",
    "        suppress_failure=False,\n",
    "        worker_debug=True,\n",
    "        worker_logdir_root='/nfs/scratch/dcde1000006/parsl_scripts/logs',\n",
    "        worker_port_range=(50000, 51000),\n",
    "        #worker_ports=None,\n",
    "        working_dir='/nfs/scratch/dcde1000006/parsl_scripts'\n",
    "    )],\n",
    "    lazy_errors=True,\n",
    "    monitoring=None,\n",
    "    retries=0,\n",
    "    run_dir='runinfo',\n",
    "    strategy='simple',\n",
    "    usage_tracking=False\n",
    ")\n",
    "\n",
    "print(\"ORNL Parsl config loaded.\")\n",
    "\n",
    "ANL_EP = '57b72e31-9f22-11e8-96e1-0a6d4e044368'\n",
    "BNL_EP = '23f78cc8-41e0-11e9-a618-0a54e005f950'\n",
    "EMSL_EP = 'e133a52e-6d04-11e5-ba46-22000b92c6ec'\n",
    "ORNL_EP = '57230a10-7ba2-11e7-8c3b-22000b9923ef'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Globus Auth for File Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "from globus_sdk import (NativeAppAuthClient, TransferClient,\n",
    "                        RefreshTokenAuthorizer, TransferData)\n",
    "from globus_sdk.exc import GlobusAPIError\n",
    "\n",
    "authout = subprocess.run(['/usr/local/anaconda3/bin/parsl-globus-auth'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "print (authout.stdout)\n",
    "print (authout.stderr)\n",
    "# Perform a Globus directory transfer, reusing refresh tokens we've already obtained for PARSL.\n",
    "# Note this is NOT a PARSL transfer\n",
    "\n",
    "\n",
    "def load_tokens_from_file(filepath):\n",
    "    \"\"\"Load a set of saved tokens.\"\"\"\n",
    "    with open(filepath, 'r') as f:\n",
    "        tokens = json.load(f)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def save_tokens_to_file(filepath, tokens):\n",
    "    \"\"\"Save a set of tokens for later use.\"\"\"\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(tokens, f)\n",
    "\n",
    "\n",
    "def update_tokens_file_on_refresh(token_response):\n",
    "    \"\"\"\n",
    "    Callback function passed into the RefreshTokenAuthorizer.\n",
    "    Will be invoked any time a new access token is fetched.\n",
    "    \"\"\"\n",
    "    save_tokens_to_file(globus_tokens, token_response.by_resource_server)\n",
    "\n",
    "dcde_parsl_client_id = '8b8060fd-610e-4a74-885e-1051c71ad473'\n",
    "\n",
    "globus_tokens='/home/dcde1000006/.parsl/.globus.json'\n",
    "\n",
    "# First authorize using those refresh tokens:\n",
    "\n",
    "try:\n",
    "    tokens = load_tokens_from_file(globus_tokens)\n",
    "\n",
    "except:\n",
    "    print(\"Valid refresh tokens not found in {}.  Unable to authorize to Globus.  Exiting!\".format(globus_tokens))\n",
    "    sys.exit(-1)\n",
    "\n",
    "\n",
    "transfer_tokens = tokens['transfer.api.globus.org']\n",
    "\n",
    "try:\n",
    "    auth_client = NativeAppAuthClient(client_id=dcde_parsl_client_id)\n",
    "except:\n",
    "    print (\"ERROR: Globus NativeAppAuthClient() call failed!  Unable to obtain a Globus authorizer!\")\n",
    "    sys.exit(-1)\n",
    "\n",
    "authorizer = RefreshTokenAuthorizer(\n",
    "    transfer_tokens['refresh_token'],\n",
    "    auth_client,\n",
    "    access_token=transfer_tokens['access_token'],\n",
    "    expires_at=transfer_tokens['expires_at_seconds'],\n",
    "    on_refresh=update_tokens_file_on_refresh)\n",
    "\n",
    "try:\n",
    "    tc = TransferClient(authorizer=authorizer)\n",
    "except:\n",
    "    print (\"ERROR: TransferClient() call failed!  Unable to call the Globus transfer interface with the provided auth info!\")\n",
    "    sys.exit(-1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show Data Set at BNL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 18M\r\n",
      "drwxr-sr-x 3 dcde1000006 root 4.0K Nov 17 17:50 parsl-outputs\r\n",
      "-rw-rw-r-- 1 dcde1000006 root   94 Nov  7 01:35 class3d-check.sh\r\n",
      "-rw-r--r-- 1 dcde1000006 root  13K Nov  7 01:35 default_pipeline.star\r\n",
      "drwxr-sr-x 3 dcde1000006 root 4.0K Nov  7 01:35 betagal\r\n",
      "-rw-r--r-- 1 dcde1000006 root 2.5M Nov  7 01:35 3i3e_ali.pdb\r\n",
      "-rw-r--r-- 1 dcde1000006 root  15M Nov  7 01:35 Maux.spi\r\n",
      "drwxr-sr-x 2 dcde1000006 root 4.0K Nov  6 13:56 Micrographs\r\n",
      "drwxr-sr-x 3 dcde1000006 root 4.0K Nov  3 22:50 Sort\r\n",
      "drwxr-sr-x 7 dcde1000006 root 4.0K Nov  3 22:50 Select\r\n",
      "drwxrwsr-x 4 dcde1000006 root 4.0K Nov  3 22:50 Refine3D\r\n",
      "drwxrwsr-x 5 dcde1000006 root 4.0K Nov  3 22:50 PostProcess\r\n",
      "drwxrwsr-x 3 dcde1000006 root 4.0K Nov  3 22:50 Polish\r\n",
      "drwxrwsr-x 3 dcde1000006 root 4.0K Nov  3 22:50 MovieRefine\r\n",
      "drwxr-sr-x 3 dcde1000006 root 4.0K Nov  3 22:50 MotionCorr\r\n",
      "drwxrwsr-x 3 dcde1000006 root 4.0K Nov  3 22:50 MaskCreate\r\n",
      "drwxr-sr-x 3 dcde1000006 root 4.0K Nov  3 22:50 ManualPick\r\n",
      "drwxrwsr-x 3 dcde1000006 root 4.0K Nov  3 22:50 LocalRes\r\n",
      "drwxr-sr-x 4 dcde1000006 root 4.0K Nov  3 22:50 InitialModel\r\n",
      "drwxrwsr-x 3 dcde1000006 root 4.0K Nov  3 22:50 Import\r\n",
      "drwxr-sr-x 4 dcde1000006 root 4.0K Nov  3 22:50 Extract\r\n",
      "drwxr-sr-x 3 dcde1000006 root 4.0K Nov  3 22:50 CtfFind\r\n",
      "drwxrwsr-x 3 dcde1000006 root 4.0K Nov  3 22:50 Class3D\r\n",
      "drwxr-sr-x 4 dcde1000006 root 4.0K Nov  3 22:50 Class2D\r\n",
      "drwxr-sr-x 4 dcde1000006 root 4.0K Nov  3 22:50 AutoPick\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lht /hpcgpfs01/scratch/dcde1000006/sc19-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Import Job at BNL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job setup: stdout = /sdcc/u/dcde1000006/parsl_scripts/relion-bnl-import.out\n",
      "stderr = /sdcc/u/dcde1000006/parsl_scripts/relion-bnl-import.err\n",
      "relion_import() invoked, now waiting...\n",
      "relion_import() invoked has finished, output should print now:\n",
      "working directory: /hpcgpfs01/scratch/dcde1000006/sc19-data\n",
      "Wrote file :\n",
      "\n",
      "data_\n",
      "loop_\n",
      "_rlnMicrographMovieName\n",
      "Micrographs/Falcon_2012_06_12-14_33_35_0_movie.mrcs\n",
      "Micrographs/Falcon_2012_06_12-14_57_34_0_movie.mrcs\n",
      "Micrographs/Falcon_2012_06_12-15_14_01_0_movie.mrcs\n",
      "Micrographs/Falcon_2012_06_12-15_41_22_0_movie.mrcs\n",
      "Micrographs/Falcon_2012_06_12-15_53_09_0_movie.mrcs\n",
      "Micrographs/Falcon_2012_06_12-15_56_10_0_movie.mrcs\n",
      "Micrographs/Falcon_2012_06_12-16_26_22_0_movie.mrcs\n",
      "Micrographs/Falcon_2012_06_12-16_44_07_0_movie.mrcs\n",
      "Micrographs/Falcon_2012_06_12-16_55_40_0_movie.mrcs\n",
      "Micrographs/Falcon_2012_06_12-16_59_12_0_movie.mrcs\n",
      "Micrographs/Falcon_2012_06_12-17_02_43_0_movie.mrcs\n",
      "Micrographs/Falcon_2012_06_12-17_14_17_0_movie.mrcs\n",
      "Micrographs/Falcon_2012_06_12-17_17_05_0_movie.mrcs\n",
      "Micrographs/Falcon_2012_06_12-17_23_32_0_movie.mrcs\n",
      "Micrographs/Falcon_2012_06_12-17_26_54_0_movie.mrcs\n",
      "\n",
      "parsl done() call returned True.  Trying to shut down executor...\n"
     ]
    }
   ],
   "source": [
    "parsl.clear()\n",
    "\n",
    "#parsl.set_stream_logger()\n",
    "parsl.load(bnl_config)\n",
    "# Note: clear(), load(), dfk() are in DataFlowKernelLoader (dflow.py)\n",
    "bnl_dfk = parsl.dfk()\n",
    "\n",
    "@bash_app\n",
    "def relion_import(job_dir=None, stdout=None, stderr=None, mock=False):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    mock : (Bool)\n",
    "       when mock=True\n",
    "    \"\"\"\n",
    "    cmd_line = '''#!/bin/bash -l\n",
    "\n",
    "export DATAROOT=/hpcgpfs01/scratch/dcde1000006/sc19-data\n",
    "export RELION_SIMG=/sdcc/u/dcde1000006/relion_singv26.simg\n",
    "\n",
    "export MOVIESTAR=Import/job001/movies.star\n",
    "export INSTAR=CtfFind/job003/micrographs_ctf.star\n",
    "export REFSTAR=Select/job007/class_averages.star\n",
    "export PICKDIR=AutoPick/job010/\n",
    "\n",
    "cd ${{DATAROOT}}\n",
    "echo -n \"working directory: \"\n",
    "pwd\n",
    "set -v\n",
    "\n",
    "singularity exec -W ${{DATAROOT}} -B /hpcgpfs01:/hpcgpfs01 ${{RELION_SIMG}} relion_star_loopheader rlnMicrographMovieName > ${{MOVIESTAR}}\n",
    "singularity exec -W ${{DATAROOT}} -B /hpcgpfs01:/hpcgpfs01 ${{RELION_SIMG}} ls Micrographs/*.mrcs >> ${{MOVIESTAR}}\n",
    "\n",
    "echo \"Wrote file ${{MOVISTAR}}:\"; echo\n",
    "cat ${{MOVIESTAR}}\n",
    "\n",
    "    '''\n",
    "    if mock:\n",
    "        return '''tmp_file=$(mktemp);\n",
    "cat<<EOF > $tmp_file\n",
    "{}\n",
    "EOF\n",
    "cat $tmp_file\n",
    "        '''.format(cmd_line)\n",
    "    else:\n",
    "        return cmd_line\n",
    "\n",
    "\n",
    "relion_stdout=os.path.join(bnl_config.executors[0].working_dir, 'relion-bnl-import.out')\n",
    "relion_stderr=os.path.join( bnl_config.executors[0].working_dir, 'relion-bnl-import.err')\n",
    "\n",
    "local_logdir='/hpcgpfs01/scratch/dcde1000006/sc19-data/parsl-outputs'\n",
    "local_logfile=os.path.join(local_logdir, 'relion-bnl-import.out')\n",
    "\n",
    "try:\n",
    "    os.remove(relion_stdout)\n",
    "except OSError:\n",
    "    pass\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "try:\n",
    "    os.remove(relion_stderr)\n",
    "except OSError:\n",
    "    pass\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "try:\n",
    "    os.remove(local_logfile)\n",
    "except OSError:\n",
    "    pass\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "\n",
    "print ('job setup: stdout = {}\\nstderr = {}'.format(relion_stdout,relion_stderr))\n",
    "# parsl.set_stream_logger()\n",
    "\n",
    "x = relion_import(job_dir=bnl_config.executors[0].working_dir, stdout=relion_stdout, stderr=relion_stderr, mock = False)\n",
    "print('relion_import() invoked, now waiting...')\n",
    "x.result()\n",
    "print('relion_import() invoked has finished, output should print now:')\n",
    "\n",
    "# FIXME: This is still goofy,  trying to get the calls and logic right:\n",
    "#if x.done():\n",
    "#if x.result():\n",
    "if True:\n",
    "    bnl_dfk.executors['bnl-condor'].provider.channel.pull_file(relion_stdout, local_logdir)\n",
    "    with open(local_logfile, 'r') as f:\n",
    "        print(f.read())\n",
    "\n",
    "# Try to shut down if we're done\n",
    "if x.done():\n",
    "    print('parsl done() call returned True.  Trying to shut down executor...')\n",
    "    bnl_dfk.executors['bnl-condor'].shutdown()\n",
    "else:\n",
    "    print(\"Oops!  parsl done() call returned False!  For some reason we don't think we're done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show updated data set at BNL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\r\n",
      "-rw-rw-r-- 1 dcde1000006 root 816 Nov 18 17:58 movies.star\r\n",
      "-rw-rw-r-- 1 dcde1000006 root   0 Nov  7 01:35 run.err\r\n",
      "-rw-rw-r-- 1 dcde1000006 root  13 Nov  7 01:35 run.out\r\n",
      "-rw-rw-r-- 1 dcde1000006 root 241 Nov  7 01:35 note.txt\r\n",
      "-rw-rw-r-- 1 dcde1000006 root 333 Nov  7 01:35 run.job\r\n",
      "-rw-rw-r-- 1 dcde1000006 root 532 Nov  7 01:35 job_pipeline.star\r\n",
      "-rw-rw-r-- 1 dcde1000006 root 532 Nov  7 01:35 default_pipeline.star\r\n",
      "-rw-r--r-- 1 dcde1000006 root 959 Nov  7 01:35 deleted_pipeline.star\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lt /hpcgpfs01/scratch/dcde1000006/sc19-data/Import/job001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Images at Import\n",
    "\n",
    "<img src=\"../../images/import-job001.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sync data from BNL to ORNL via Globus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcep = BNL_EP\n",
    "destep = ORNL_EP\n",
    "srcdir = '/hpcgpfs01/scratch/dcde1000006/sc19-data'\n",
    "destdir =  '/nfs/data/dcde-store/scratch/sc19-data'\n",
    "\n",
    "xferlabel = \"DCDE Relion transfer BNL to ORNL\"\n",
    "\n",
    "tdata = TransferData(tc, srcep, destep,\n",
    "                     label=xferlabel,\n",
    "                     sync_level=\"mtime\")\n",
    "\n",
    "\n",
    "tdata.add_item( srcdir, destdir, recursive = True)\n",
    "    \n",
    "transfer_result = tc.submit_transfer(tdata)\n",
    "\n",
    "print(\"task_id =\", transfer_result[\"task_id\"])\n",
    "\n",
    "\n",
    "while not tc.task_wait(transfer_result['task_id'], timeout=1200, polling_interval=10):\n",
    "    print(\".\", end=\"\")\n",
    "print(\"\\n{} completed!\".format(transfer_result['task_id']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Autopick job at ORNL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsl.clear()\n",
    "\n",
    "#parsl.set_stream_logger()\n",
    "parsl.load(ornl_config)\n",
    "ornl_dfk = parsl.dfk()\n",
    "\n",
    "@bash_app\n",
    "def relion_autopick_ornl(job_dir=None, stdout=None, stderr=None, mock=True):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    mock : (Bool)\n",
    "       when mock=True\n",
    "    \"\"\"\n",
    "    cmd_line = '''#!/bin/bash -l\n",
    "\n",
    "export DATAROOT=/nfs/data/dcde-store/scratch/sc19-data\n",
    "export RELION_SIMG=/nfs/sw/relion/relion_singv26.simg\n",
    "\n",
    "export INSTAR=CtfFind/job003/micrographs_ctf.star\n",
    "export REFSTAR=Select/job007/class_averages.star\n",
    "export PICKDIR=AutoPick/job010/\n",
    "export PARTSTAR=Extract/job011/particles.star\n",
    "export PARTDIR=job011/\n",
    "\n",
    "echo -n \"working directory: \"\n",
    "pwd\n",
    "set -v\n",
    "\n",
    "singularity exec -W ${{DATAROOT}} -B ${{DATAROOT}}:${{DATAROOT}} ${{RELION_SIMG}} relion_autopick --i ${{INSTAR}} --ref ${{REFSTAR}} --odir ${{PICKDIR}} --pickname autopick --invert  --ctf  --ang 5 --shrink 0 --lowpass 20 --particle_diameter 200 --threshold 0.4 --min_distance 110 --max_stddev_noise 1.1 # --gpu \"0\"\n",
    "echo ${{INSTAR}} > AutoPick/job010/coords_suffix_autopick.star\n",
    "\n",
    "#singularity exec -B ${{DATAROOT}}:${{DATAROOT}}  ${{RELION_SIMG}} relion_preprocess --i ${{INSTAR}} --coord_dir ${{PICKDIR}} --coord_suffix _autopick.star --part_star ${{PARTSTAR}} --part_dir ${{PARTDIR}} --extract --extract_size 100 --norm --bg_radius 30 --white_dust -1 --black_dust -1 --invert_contrast\n",
    "\n",
    "    '''\n",
    "    if mock:\n",
    "        return '''tmp_file=$(mktemp);\n",
    "cat<<EOF > $tmp_file\n",
    "{}\n",
    "EOF\n",
    "cat $tmp_file\n",
    "        '''.format(cmd_line)\n",
    "    else:\n",
    "        return cmd_line\n",
    "\n",
    "\n",
    "\n",
    "relion_stdout=os.path.join(ornl_config.executors[0].working_dir, 'relion-ornl-autopick.out')\n",
    "relion_stderr=os.path.join(ornl_config.executors[0].working_dir, 'relion-ornl-autopick.err')\n",
    "\n",
    "local_logdir='/nfs/data/dcde-store/scratch/sc19-data/parsl-outputs'\n",
    "local_logfile=os.path.join(local_logdir, 'relion-ornl-autopick.out')\n",
    "\n",
    "try:\n",
    "    os.remove(relion_stdout)\n",
    "except OSError:\n",
    "    pass\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "try:\n",
    "    os.remove(relion_stderr)\n",
    "except OSError:\n",
    "    pass\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "try:\n",
    "    os.remove(local_logfile)\n",
    "except OSError:\n",
    "    pass\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "\n",
    "print ('job setup: \\nstdout = {}\\nstderr = {}'.format(relion_stdout,relion_stderr))\n",
    "# parsl.set_stream_logger()\n",
    "# Call Relion and wait for results\n",
    "\n",
    "x = relion_autopick_ornl(stdout=relion_stdout, stderr=relion_stderr, mock = True)\n",
    "print('relion_autopick_ornl() invoked, now waiting...')\n",
    "x.result()\n",
    "print('relion_autopick_ornl() returned, should print output below:')\n",
    "\n",
    "if x.done():\n",
    "    ornl_dfk.executors['ornl-slurm'].provider.channel.pull_file(relion_stdout, local_logdir)\n",
    "    with open(local_logfile, 'r') as f:\n",
    "        print(f.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sync new data back from ORNL to BNL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcep = ORNL_EP\n",
    "destep = BNL_EP\n",
    "srcdir =  '/nfs/data/dcde-store/scratch/sc19-data'\n",
    "destdir = '/hpcgpfs01/scratch/dcde1000006/sc19-data'\n",
    "\n",
    "xferlabel = \"DCDE Relion transfer ORNL to BNL\"\n",
    "\n",
    "tdata = TransferData(tc, srcep, destep,\n",
    "                     label=xferlabel,\n",
    "                     sync_level=\"mtime\")\n",
    "\n",
    "\n",
    "tdata.add_item( srcdir, destdir, recursive = True)\n",
    "    \n",
    "transfer_result = tc.submit_transfer(tdata)\n",
    "\n",
    "print(\"task_id =\", transfer_result[\"task_id\"])\n",
    "\n",
    "\n",
    "while not tc.task_wait(transfer_result['task_id'], timeout=1200, polling_interval=10):\n",
    "    print(\".\", end=\"\")\n",
    "print(\"\\n{} completed!\".format(transfer_result['task_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sync data from BNL to ANL via Globus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcep = BNL_EP\n",
    "destep = ANL_EP\n",
    "srcdir = '/hpcgpfs01/scratch/dcde1000006/sc19-data'\n",
    "destdir =  '/blues/gpfs/home/dcowley/sc19-demo'\n",
    "\n",
    "xferlabel = \"DCDE Relion transfer BNL to ANL\"\n",
    "\n",
    "\n",
    "\n",
    "tdata = TransferData(tc, srcep, destep,\n",
    "                     label=xferlabel,\n",
    "                     sync_level=\"mtime\")\n",
    "\n",
    "\n",
    "tdata.add_item( srcdir, destdir, recursive = True)\n",
    "    \n",
    "transfer_result = tc.submit_transfer(tdata)\n",
    "\n",
    "print(\"task_id =\", transfer_result[\"task_id\"])\n",
    "\n",
    "\n",
    "while not tc.task_wait(transfer_result['task_id'], timeout=1200, polling_interval=10):\n",
    "    print(\".\", end=\"\")\n",
    "print(\"\\n{} completed!\".format(transfer_result['task_id']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Extract job at ANL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsl.clear()\n",
    "\n",
    "parsl.load(anl_config)\n",
    "anl_dfk = parsl.dfk()\n",
    "#print(anl_dfk.executors)\n",
    "\n",
    "@bash_app\n",
    "def relion_extract(job_dir=None, stdout=None, stderr=None, mock=True):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    mock : (Bool)\n",
    "       when mock=True\n",
    "    \"\"\"\n",
    "    cmd_line = '''#!/bin/bash -l\n",
    "\n",
    "export I_MPI_FABRICS=shm:tmi\n",
    "\n",
    "export DATAROOT=/blues/gpfs/home/dcowley/relion-bootstrap\n",
    "export RELION_SIMG=/lcrc/project/DCDE/relion/relion_singv26.simg\n",
    "\n",
    "export INSTAR=CtfFind/job003/micrographs_ctf.star\n",
    "export REFSTAR=Select/job007/class_averages.star\n",
    "export PICKDIR=AutoPick/job010/\n",
    "\n",
    "echo -n \"working directory: \"\n",
    "pwd\n",
    "module load singularity/2.6.0\n",
    "set -v\n",
    "\n",
    "# autopick:\n",
    "#singularity exec  -B /blues/gpfs/home:/blues/gpfs/home ${{RELION_SIMG}} relion_autopick --i ${{INSTAR}} --ref ${{REFSTAR}} --odir ${{PICKDIR}} --pickname autopick --invert  --ctf  --ang 5 --shrink 0 --lowpass 20 --particle_diameter 200 --threshold 0.4 --min_distance 110 --max_stddev_noise 1.1\n",
    "\n",
    "#extract:\n",
    "singularity exec -W ${{DATAROOT}} -B ${{DATAROOT}}:${{DATAROOT}}  ${{RELION_SIMG}} relion_preprocess --i ${{INSTAR}} --coord_dir ${{PICKDIR}} --coord_suffix _autopick.star --part_star ${{PARTSTAR}} --part_dir ${{PARTDIR}} --extract --extract_size 100 --norm --bg_radius 30 --white_dust -1 --black_dust -1 --invert_contrast\n",
    "    '''\n",
    "    if mock:\n",
    "        return '''tmp_file=$(mktemp);\n",
    "cat<<EOF > $tmp_file\n",
    "{}\n",
    "EOF\n",
    "cat $tmp_file\n",
    "        '''.format(cmd_line)\n",
    "    else:\n",
    "        return cmd_line\n",
    "\n",
    "relion_stdout=os.path.join(anl_config.executors[0].working_dir, 'relion-anl-extract.out')\n",
    "relion_stderr=os.path.join( anl_config.executors[0].working_dir, 'relion-anl-extract.err')\n",
    "\n",
    "#local_logdir='/blues/gpfs/home/dcowley/sc19-data/parsl-outputs'\n",
    "# This is local to BNL!\n",
    "local_logdir= '/hpcgpfs01/scratch/dcde1000006/sc19-data/parsl-outputs'\n",
    "\n",
    "local_logfile=os.path.join(local_logdir, 'relion-anl-extract.out')\n",
    "\n",
    "try:\n",
    "    os.remove(relion_stdout)\n",
    "except OSError:\n",
    "    pass\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "try:\n",
    "    os.remove(relion_stderr)\n",
    "except OSError:\n",
    "    pass\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "try:\n",
    "    os.remove(local_logfile)\n",
    "except OSError:\n",
    "    pass\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "print ('job setup: stdout = {}\\nstderr = {}'.format(relion_stdout,relion_stderr))\n",
    "# parsl.set_stream_logger()\n",
    "# Call Relion and wait for results\n",
    "\n",
    "x = relion_extract(stdout=relion_stdout, stderr=relion_stderr, mock = False)\n",
    "print('relion_extract() invoked, now waiting...')\n",
    "x.result()\n",
    "print('relion_extract() returned, output should print below:')\n",
    "\n",
    "#if x.done():\n",
    "if True:\n",
    "    anl_dfk.executors['anl-slurm'].provider.channel.pull_file(relion_stdout, local_logdir)\n",
    "    with open(local_logfile, 'r') as f:\n",
    "        print(f.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sync new data back from ANL to BNL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcep = ANL_EP\n",
    "destep = BNL_EP\n",
    "\n",
    "srcdir =  '/blues/gpfs/home/dcowley/sc19-demo'\n",
    "destdir = '/hpcgpfs01/scratch/dcde1000006/sc19-data'\n",
    "\n",
    "xferlabel = \"DCDE Relion transfer ANL to BNL\"\n",
    "\n",
    "\n",
    "\n",
    "tdata = TransferData(tc, srcep, destep,\n",
    "                     label=xferlabel,\n",
    "                     sync_level=\"mtime\")\n",
    "\n",
    "\n",
    "tdata.add_item( srcdir, destdir, recursive = True)\n",
    "    \n",
    "transfer_result = tc.submit_transfer(tdata)\n",
    "\n",
    "print(\"task_id =\", transfer_result[\"task_id\"])\n",
    "\n",
    "\n",
    "while not tc.task_wait(transfer_result['task_id'], timeout=1200, polling_interval=10):\n",
    "    print(\".\", end=\"\")\n",
    "print(\"\\n{} completed!\".format(transfer_result['task_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Images After Extract\n",
    "<img src=\"../../images/extract-job005.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample images after 2D classification\n",
    "<img src=\"../../images/class2d-after-sorting-job015.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample images after 3D Classification\n",
    "<img src=\"../../images/class3d-job018-oneone.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Result: 3D structure of Protein\n",
    "Data is in `/hpcgpfs01/scratch/dcde1000006/sc19-data/Class3D/job018:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /hpcgpfs01/scratch/dcde1000006/sc19-data/Class3D/job018/*.mrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nglview as nv\n",
    "m = nv.show_file('/hpcgpfs01/scratch/dcde1000006/sc19-data/Class3D/job018/run_it025_class002.mrc')\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCDE Globus Endpoints and Data Directories\n",
    "\n",
    "\n",
    "|  name |   Other  name   | UUID  | Directory |\n",
    "|  --- | ---   | ---  | --- |\n",
    "| BNL DTN | `globus02.sdcc.bnl.gov` |   `23f78cc8-41e0-11e9-a618-0a54e005f950` | `/hpcgpfs01/scratch/dcde1000006/sc19-data` |\n",
    "| LCRC DTN | `bmgt3.lcrc.anl.gov` | `57b72e31-9f22-11e8-96e1-0a6d4e044368` |  `/blues/gpfs/home/dcowley/sc19-demo` |\n",
    "| ORNL DTN | `CADES-OR` |  `57230a10-7ba2-11e7-8c3b-22000b9923ef` | `/nfs/data/dcde-store/scratch/sc19-data`  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DCDE_RX",
   "language": "python",
   "name": "dcde_rx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
